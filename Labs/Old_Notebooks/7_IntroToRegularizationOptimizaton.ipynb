{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Regularization for Deep Neural Nets\n",
    "\n",
    "## Stephen Elston\n",
    "\n",
    "\n",
    "This lesson will introduce you to the principles of regularization required to successfully train deep neural networks. In this lesson you will:\n",
    "\n",
    "1. Understand the need for regularization of complex machine learning models, particularly deep NNs. \n",
    "2. Know how to apply constraint-based regularization using the L1 and L2 norms.\n",
    "3. Understand and apply the concept of data augmentation. \n",
    "4. Know how to apply dropout regularization. \n",
    "5. Understand and apply early stopping. \n",
    "6. Understand the advantages of various regularization methods and know when how to apply them in combination. \n",
    "\n",
    "\n",
    "****\n",
    "**Note:** To run notebook you must have Keras package installed, In addition you will need the `hdf5` package installed. You can install this package using either anaconda or pip from a command prompt, as follows:\n",
    "\n",
    "`conda install hdf5`\n",
    "\n",
    "Or,\n",
    "\n",
    "`pip install hdf5`\n",
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 1.0 Why do we need regularization for deep learning?\n",
    "\n",
    "Deep learning models have a great many parameters (weights) which must be fit. This situation arises from the wide and deep architectures that are required to achieve significant **model capacity** for representing complex functions. The core issue is that over-fit models will simply learn the training data and **over-fit models do not generalize**. Therefore, regularization methods are required in order to prevent over-fitting.\n",
    "\n",
    "In particular, we can point to three interrelated problems with training deep neural networks:\n",
    "\n",
    "1. Neural network models have large numbers of parameters (weights). With any finite size data set, there is likely to be a low ratio of cases per parameter or low ratio of cases to features. For example, if we are classifying 512 x 512 images with 1,000,000 training cases there are only about 4 cases per feature. This is why training neural networks often requires massive amounts of data. \n",
    "2. As a result of the large numbers of parameters, neural networks are susceptible to noise in the training data. Neural networks are generally considered less robust to noise than shallow machine learning methods. \n",
    "3. Presumably as a result of the model complexity, neural networks often return unexpected predictions for data cases outside the training data domain. This property has been referred to as **brittleness**. Brittleness has proven to be serious problem in some production systems. \n",
    "\n",
    "The regularization methods presented here will limit these effects. However, there is no 'silver bullet'! Neural networks are hard to train under the best of circumstances. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Bias-variance trade-off\n",
    "\n",
    "When applying regularization one must come to terms with the **bias-variance trade-off**. Here are some simple examples of extreme cases:\n",
    "\n",
    "- If we say that our prediction for all cases is just the mean (or median), we have minimized the variance. The estimate for all cases is the same, so the bias of the estimates is zero. However, there is likely considerable variance in these estimates. \n",
    "- On the other hand, we can fit the same data with a kNN model with k=1. The training data will fit this model perfectly, since there  is one model coefficient per training data point. The variance will be high. On the other had the model will have low bias when applied to test data. \n",
    "\n",
    "In either case, these extreme models will not generalize well and will exhibit large errors on any independent test data. Any practical model must come to terms with the trade-off between bias and variance to make accurate predictions. \n",
    "\n",
    "To better understand this trade-off let's decompose mean square error for a model as follows:\n",
    "\n",
    "$$\\Delta y = E \\big[ Y - \\hat{f}(X) \\big]$$\n",
    "\n",
    "Where,     \n",
    "$Y = $ the label vector.  \n",
    "$X = $ the feature matrix.   \n",
    "$\\hat{f}(x) = $ the trained model.   \n",
    "\n",
    "Expanding this relation gives us:\n",
    "\n",
    "$$\\Delta y = \\big( E[ \\hat{f}(X)] - \\hat{f}(X) \\big)^2 + E \\big[ ( \\hat{f}(X) - E[ \\hat{f}(X)])^2 \\big] + \\sigma^2\\\\\n",
    "\\Delta y = Bias^2 + Variance + Irreducible\\ Error$$\n",
    "\n",
    "\n",
    "\n",
    "Regularization will reduce variance, but increase bias. Regularization parameters must be chosen to minimize minimize $\\Delta x$. In many cases, this will prove challenging. \n",
    "\n",
    "Notice that the **irreducible error** is the limit of model accuracy. Even if we had a perfect model with no bias or variance, the irreducible error is inherent in the data and problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Demonstration of over-parameterization\n",
    "\n",
    "Let's try a simple example. We will construct a regression models with different numbers of parameters and therefore different model capacities. \n",
    "\n",
    "As a first step, we will create a simple single regression model of some synthetic data. The code in the cell below creates data computed from as a straight line, but with considerable Normally distributed random noise. A plot is then created of the result. Execute this code and examine the resulting plot.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import numpy.random as nr\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.random import normal, seed\n",
    "import sklearn.linear_model as slm\n",
    "from sklearn.preprocessing import scale\n",
    "import sklearn.model_selection as ms\n",
    "from math import sqrt\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import keras.models as models\n",
    "import keras.layers as layers\n",
    "from keras.layers import Dropout, LeakyReLU, BatchNormalization\n",
    "from keras import optimizers\n",
    "from tensorflow.keras import regularizers\n",
    "#from keras import regularizationregularizers\n",
    "#from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "seed(34567)\n",
    "x = np.arange(start = 0.0, stop = 10.0, step = 0.25) \n",
    "y = np.add(x, normal(scale = 2.0, size = x.shape[0]))\n",
    "\n",
    "plt.scatter(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that thees data points fall approximately on a straight line, but with significant deviations. \n",
    "\n",
    "Next, we will compute a simple single regression model. This model has an intercept term and a single slope parameter. The code in the cell below splits the data into randomly selected training and testing subsets. Execute this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indx = range(len(x))\n",
    "seed(9988)\n",
    "indx = ms.train_test_split(indx, test_size = 20)\n",
    "x_train = np.ravel(x[indx[0]])\n",
    "y_train = np.ravel(y[indx[0]])\n",
    "x_test = np.ravel(x[indx[1]])\n",
    "y_test = np.ravel(y[indx[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will use the linear model in `sklearn.linear_model` package to create a single regression model for these data. The code in the cell below does just this, prints the single model coefficient, and plots the result. Execute this code. \n",
    "\n",
    "***\n",
    "> **Note:** you can find documentation along with some examples of scikit-learn regression models on the [regression models pages](http://scikit-learn.org/stable/supervised_learning.html#supervised-learning).\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_reg(x, y_hat, y):\n",
    "    ax = plt.figure(figsize=(6, 6)).gca() # define axis\n",
    "\n",
    "    ## Get the data in plot order\n",
    "    x_yhat_y = sorted(zip(x, y_hat, y))\n",
    "    x = [x for x, _, _ in x_yhat_y]\n",
    "    y_hat = [y_hat for _, y_hat, _ in x_yhat_y]\n",
    "    y = [y for _, _, y in x_yhat_y]\n",
    "    \n",
    "    ## Plot the result\n",
    "    plt.plot(x, y_hat, c = 'red')\n",
    "    plt.scatter(x, y)\n",
    "    plt.show()    \n",
    "\n",
    "def reg_model(x, y):\n",
    "    mod = slm.LinearRegression()\n",
    "    x_scale = scale(x)  # .reshape(-1, 1)\n",
    "    mod.fit(x_scale, y)\n",
    "    print(mod.coef_)\n",
    "    return mod, x_scale, mod.predict(x_scale)\n",
    "\n",
    "mod, x_scale, y_hat = reg_model(x_train.reshape(-1, 1), y_train)\n",
    "\n",
    "plot_reg(x_scale, y_hat, y_train)\n",
    "print('RMSE = ' + str(np.std(y_hat - y_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine these results. Notice that the single coefficient (slope) seems reasonable, given the standardization of the training data. Visually, the fit to the training data also looks reasonable. \n",
    "\n",
    "We should also test the fit to some test data. The code in the cell does just this and returns the RMS error. execute this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "def test_mod(x,y, mod):\n",
    "    x_scale = scale(x)\n",
    "    y_score = mod.predict(x_scale)\n",
    "    plot_reg(x_scale, y_score, y)\n",
    "    plt.show()\n",
    "    print('RMSE = '+ str(np.std(y_score - y))) \n",
    "\n",
    "test_mod(x_test.reshape(-1, 1), y_test, mod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, these results look reasonable. The RMSE is relatively small given the significant dispersion in these data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 7-1:** You will try a model with significantly higher capacity. In this case the new features are a 9th order polynomial. Using this new set of features you will train a regression model and display a summary of the results. Now do the following:    \n",
    "> 1. Use the `x_scale` feature set for model training.     \n",
    "> 2. Define a Scikit-Learn [LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) object named `mod_power`.   \n",
    "> 3. Apply the `fit` method to the linear regression object using the `x_scale` features and `y_train` labels.    \n",
    "> 4. Apply the `predict` method to the trained model with the `x_scale` features.  \n",
    "> 5. Use the `plot_reg` to display the fit of the model to the training data.  \n",
    "> 6. Print the model coefficients and RMSE of the model fit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def test_mod_multi(x, features, y, mod):\n",
    "    y_score = mod.predict(features)\n",
    "    plot_reg(x, y_score, y)\n",
    "    print('RMSE = '+ str(np.std(y_score - y)))\n",
    "\n",
    "seed(2233)\n",
    "x_power = np.power(x_train.reshape(-1, 1), range(1,10))\n",
    "x_scale = scale(x_power)\n",
    "print('The shape of the feature set = ' + str(x_scale.shape))\n",
    "\n",
    "## Your code below\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Examine these results and compare them to the results of the foregoing trained single feature regression model. \n",
    "> 1. Compare the graphs of the fit of the polynomial model to the single feature model. What does the difference tell you about model capacity and bias?  \n",
    "> 2. Compare the training RMSE of the polynomial model to the single regression models. What does this difference tell you about training bias, and why? \n",
    "> 3. Compare the values of the model coefficients of the polynomial model with the single feature model. What does the difference in magnitudes tell you about the likely generalization? \n",
    "> **End of exercise**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answers:**\n",
    "> 1.   \n",
    "> 2.  \n",
    "> 3.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will try to test the model with the held-back test data. The code in the cell below creates the same features and applies the `predict` method to the model using these test features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x_test_scale = scale(x_test.reshape(-1, 1)) # Prescale to prevent numerical overflow. \n",
    "x_test_power = np.power(x_test_scale, range(1,10))\n",
    "x_scale_test = scale(x_test_power)\n",
    "\n",
    "y_hat_power = mod_power.predict(x_scale_test)\n",
    "\n",
    "test_mod_multi(x_test, x_scale_test, y_test, mod_power)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 7-2:** This is clearly a terrible fit! Examine this result and answer the following questions:   \n",
    "> 1. Compare the graph of the fit of the test data for the polynomial model with the graph for the single parameter model. What does the difference in these plots tell you about the generalization of these models.  \n",
    "> 2. Compare the RMSE of the polynomial model to the RMSE of the single feature model. What does the difference in values tell you about the generalization of these models? \n",
    "> **End of exercise**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answers:**   \n",
    "> 1.    \n",
    "> 2.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 l2 regularization\n",
    "\n",
    "We will now explore one of the mostly widely used regularization methods, often referred to as l2 regularization. \n",
    "\n",
    "The same method goes by some other names, as it has been 'invented' several times. In particular this method is known as, **Tikhonov regularization**, **l2 norm regularization**, **pre-whitening** in engineering, and for linear models **ridge regression**. In all likelihood the method was first developed by the Russian mathematician Andrey Tikhonov in the late 1940's. His work was not widely known in the West since his short book on the subject, [Solution of Ill-Posed Problems](https://www.researchgate.net/publication/44438630_Solutions_of_ill-posed_problems_Andrey_N_Tikhonov_and_Vasiliy_Y_Arsenin), was only published in English in 1977, about 30 years after it had appeared in Russian.\n",
    "\n",
    "![](img/Tikhonov_board.jpg)\n",
    "<center> **Figure 2.1   \n",
    "Commemorative plaque for Andrey Nikolayevich Tikhonov at Moscow State University**\n",
    "\n",
    "\n",
    "So, what is the basic idea? l2 regularization applies a **penalty** proportional to the **l2** or **Euclidean norm** of the model weights to the loss function. The total loss function then becomes:  \n",
    "\n",
    "$$J(W) = J_{MLE}(W) + \\lambda ||W||^2$$\n",
    "\n",
    "Where,\n",
    "\n",
    "$$||W||^2 = \\big( w_1^2 + w_2^2 + \\ldots + w_n^2 \\big)^{\\frac{1}{2}} = \\Big( \\sum_{i=1}^n w_i^2 \\Big)^{\\frac{1}{2}}$$\n",
    "\n",
    "We call $||W||^2$ the l2 norm of the weights since we square the power of the weights, sum, and then take the square root, or $\\frac{1}{2}$ power. \n",
    "\n",
    "You can think of this penalty as constraining the 12 or Euclidean norm of the model weight vector. The value of the hyperparameter $\\lambda$ determines how much the norm of the coefficient vector constrains the solution. You can see a view of this geometric interpretation in Figure 2.2 below.  \n",
    "\n",
    "![](img/L2.jpg)\n",
    "<center> **Figure 2.2. Geometric view of l2 regularization**\n",
    "\n",
    "Notice that for a constant value of l2, the values of the model parameters $B1$ and $B2$ are related. For example, if $B1$ is maximized then $B2 \\sim 0$, or vice versa. It is important to note that l2 regularization is a **soft constraint**. Coefficients are driven close to, but likely not exactly to, zero.   \n",
    "\n",
    "****\n",
    "**What is the difference between the L2 norm and l2 norm?**  Notice that we are discussing the l2 norm with a lower case 'l'. You will often find discussion of the L2 norm with an upper case 'L', or where the l2 and L2 notation are used interchangeably. In practice the difference can be subtle and is often ignored. \n",
    "\n",
    "We have already defined the l2 norm as the square root of the sum of the squares of a discrete variable (e.g. a set of weight values). Strictly speaking the L2 norm is the equivalent metric for a continuous function. For example, we can write express the L2 norm of a continuous function $\\phi(x)$ as: \n",
    "\n",
    "$$||\\phi(x)||^2 = \\langle \\phi(x), \\phi(x) \\rangle = \\int \\phi(x)^2 dx$$\n",
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 The eigenvalue penalty interpretation\n",
    "\n",
    "To develop deeper understanding of l2 regularization we look at the eigenvalue-eigenvector decomposition for a linear model. While neural networks are clearly nonlinear, the behavior is approximately linear locally, particularly near a minimum of the loss function.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 Review of Eigenvalue Decomposition\n",
    "\n",
    "**Eigenvalues** are characteristic roots or characteristic values of a linear system of equations. The **eigenvalue-eigenvector** decomposition is a factorization of the a matrix. \n",
    "\n",
    "Let's start with a **square matrix**, $A$:\n",
    "\n",
    "$$A = \n",
    "\\begin{bmatrix}\n",
    "   a_{11}  & a_{12} & \\dots & a_{1n} \\\\\n",
    "    a_{21}  & a_{22} & \\dots & a_{2n} \\\\\n",
    "    \\vdots &\\vdots &\\vdots & \\vdots \\\\\n",
    "    a_{n1} & a_{n2} &  \\dots & a_{nn}\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "Next define a vector, $x$: \n",
    "\n",
    "$$x = \n",
    "\\begin{bmatrix}\n",
    "   x_{1}\\\\\n",
    "    x_{2}\\\\\n",
    "    \\vdots\\\\\n",
    "    x_{n}\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "Then an **eigenvalue** of the matrix $A$ has the property: \n",
    "\n",
    "$$A x = \\lambda x$$\n",
    "\n",
    "Or,   \n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "   a_{11}  & a_{12} & \\dots & a_{1n} \\\\\n",
    "    a_{21}  & a_{22} & \\dots & a_{2n} \\\\\n",
    "    \\vdots &\\vdots &\\vdots & \\vdots \\\\\n",
    "    a_{n1} & a_{n2} &  \\dots & a_{nn}\n",
    "\\end{bmatrix}  \n",
    "\\begin{bmatrix}\n",
    "   x_{1}\\\\\n",
    "    x_{2}\\\\\n",
    "    \\vdots\\\\\n",
    "    x_{n}\n",
    "\\end{bmatrix} \n",
    "= \n",
    "\\lambda \n",
    "\\begin{bmatrix}\n",
    "   x_{1}\\\\\n",
    "    x_{2}\\\\\n",
    "    \\vdots\\\\\n",
    "    x_{n}\n",
    "\\end{bmatrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "To see that the eigenvalue, $\\lambda$, is a root of the matrix, $A$ we can rearrange the above as follows:   \n",
    "\n",
    "\\begin{align}\n",
    "Ax - \\lambda x &= 0 \\\\\n",
    "(A - I \\lambda) x &= 0\n",
    "\\end{align}\n",
    "\n",
    "Where, $I$ is the **identity matrix** of 1 on the diagonal and 0 elsewhere. These relationships can be written as follows:  \n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "   a_{11} - \\lambda  & a_{12} & \\dots & a_{1n} \\\\\n",
    "    a_{21}  & a_{22} - \\lambda  & \\dots & a_{2n} \\\\\n",
    "    \\vdots &\\vdots &\\vdots & \\vdots \\\\\n",
    "    a_{n1} & a_{n2} &  \\dots & a_{nn} - \\lambda \n",
    "\\end{bmatrix}  \n",
    "\\begin{bmatrix}\n",
    "   x_{1}\\\\\n",
    "    x_{2}\\\\\n",
    "    \\vdots\\\\\n",
    "    x_{n}\n",
    "\\end{bmatrix} \n",
    "= \n",
    "\\begin{bmatrix}\n",
    "   x_{1}\\\\\n",
    "    x_{2}\\\\\n",
    "    \\vdots\\\\\n",
    "    x_{n}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "The foregoing show that the eigenvalue, $\\lambda$, is a root of the matrix, $A$. Further, notice that the eigin \n",
    "\n",
    "For an $n\\ x\\ n$ matrix, $A$, there are $n$ eigenvalues or roots. These can be found by solving the following equation, using the determinant:  \n",
    "\n",
    "$$det(A - x) = 0$$\n",
    "\n",
    "You can find more information on the determinant in this [article](https://en.wikipedia.org/wiki/Determinant)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eigenvalues and Eigenvectors\n",
    "\n",
    "There are special vectors, known as an **eigenvectors**, associated with each eigenvalue of a square matrix, $A$. There is, in fact, a left and right eigenvector for each eigenvalue. \n",
    "\n",
    "Let's start with a **right eigenvector** which has the property: \n",
    "\n",
    "\\begin{align}\n",
    " A x_r &= \\lambda_r x_r\\\\\n",
    "(A - I \\lambda_r)x_r &= 0\n",
    "\\end{align}\n",
    "\n",
    "Notice that the right eigenvector, $\\lambda_r$, is to the right of the matrix, $A$, giving rise to the name.   \n",
    "\n",
    "The relationship for the **left eigenvector** is a bit more complicated, To get the same result as shown for the right eigenvector it is necessary to take a transpose of the system of equations: \n",
    "\n",
    "\\begin{align}\n",
    "x_l A  &= \\lambda_l x_l\\\\\n",
    "(x_l A)^T  &= \\lambda_l x_l^T\\\\\n",
    "(A^T - I \\lambda_l)x_l^T &= 0\n",
    "\\end{align}\n",
    "\n",
    "For the above relationships to hold, the eigenvectors must have unit l2 norm:\n",
    "\n",
    "$$||x||^2 = (x_{1}^2 + x_{2}^2 \\ldots x_{n}^2)^{1/2} = 1$$\n",
    "\n",
    "Using the above relationships we can factor a square $n\\ x\\ n$ matrix, $A$ into eigenvectors and eigenvalues. Representing the matrix of eigenvectors as $Q$ and the diagonal matrix of n eigenvalues as $\\Lambda$:\n",
    "\n",
    "$$A = Q \\Lambda Q^{-1}$$\n",
    "\n",
    "Where $Q^{-1}$ is the inverse of the eigenvector matrix. Since the eigenvectors have norm 1, the eigenvector matrices, $Q$ are then **unitary**.  \n",
    "\n",
    "Using the eigenvalue-eigenvector decompositions, the inverse of a square $n\\ x\\ n$ matrix can be expressed:\n",
    "\n",
    "$$A^{-1} = Q \\Lambda^{-1} Q^{-1}$$  \n",
    "\n",
    "In fact, since the matrix of eigenvectors is unitary, we can compute the value of the matrix $A$ to the Nth power as:\n",
    "\n",
    "$$A^{N} = Q \\Lambda^{N} Q^{-1}$$\n",
    "\n",
    "You can find more information on the properties of eigenvalue-eigenvector decomposition in this [article](https://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3 Eigenvalues and the Normal Equations\n",
    "\n",
    "Let's start by examining the **normal equation** formulation of the linear regression problem. The goal is to compute a vector of **model coefficients** or weights which minimize the mean squared residuals, given a vector of data $x$ and a **model matrix** $A$. We can write our model as:\n",
    "\n",
    "$$x = A b$$\n",
    "\n",
    "To solve this problem we would ideally like to compute:\n",
    "\n",
    "$$b = A^{-1}x$$\n",
    "\n",
    "The commonly used normal equation form can help with this problem:\n",
    "\n",
    "$$b = (A^TA)^{-1}A^Tx$$\n",
    "\n",
    "Now, $A^TA$ is a symmetric $m x m$ covariance matrix, where $m$ is the number of model coefficients. This is a significant reduction in size when compared to $A$. \n",
    "\n",
    "Now, we can perform eigenvalue-eigenvector decomposition of $A^TA$:\n",
    "\n",
    "$$A^TA = Q \\Lambda Q^{-1}$$\n",
    "\n",
    "Where,\n",
    "$Q = $ unitary matrix of orthonormal **eigenvectors**, and\n",
    "$\\Lambda =$ diagonal matrix of **eigenvalues**. The eigenvalue matrix is diagonal:  \n",
    "\n",
    "$$\\Lambda = \n",
    "\\begin{bmatrix}\n",
    "    \\lambda_1  & 0 & 0 & \\dots & 0 \\\\\n",
    "    0  & \\lambda_2 & 0 & \\dots & 0 \\\\\n",
    "    \\vdots &\\vdots &\\vdots & & \\vdots \\\\\n",
    "    0 & 0 & 0 & \\dots & \\lambda_n\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "\n",
    "Since Q is unitary (unit norm), the inverse of $A^TA$ is easily computed:\n",
    "\n",
    "$$(A^TA)^{-1} = Q \\Lambda^{-1} Q^{-1}$$\n",
    "\n",
    "Where,\n",
    "$$\\Lambda^{-1} = \n",
    "\\begin{bmatrix}\n",
    "    \\frac{1}{\\lambda_1}  & 0 & 0 & \\dots & 0 \\\\\n",
    "    0  & \\frac{1}{\\lambda_2} & 0 & \\dots & 0 \\\\\n",
    "    \\vdots &\\vdots &\\vdots & & \\vdots \\\\\n",
    "    0 & 0 & 0 & \\dots & \\frac{1}{\\lambda_n}\n",
    "\\end{bmatrix}$$\n",
    "and $\\lambda_i$ is the ith eigenvalue. \n",
    "\n",
    "But, **$A^TA$ can still be rank deficient!** By rank deficient we mean that there are fewer non-zero eigenvalues of $A^TA$ than the dimension, $n$. Even is the ith eigenvalue is close to zero, $\\frac{1}{\\lambda_i}$ becomes very large and destabilizes the inverse. \n",
    "\n",
    "The basic idea of $l_2$ regularization, Tikhonov regularization, or ridge regression is to stabilize the inverse eigenvalue matrix,$\\Lambda$, by **adding a small bias term**, $\\alpha$, to each of the eigenvalues. We can write this operation in matrix notation as follows. We start with a modified form of the normal equations (also know as the **L2 or Euclidean norm** minimization problem):\n",
    "\n",
    "$$min [\\parallel A \\cdot x - b \\parallel + \\parallel \\alpha^2 \\cdot I \\parallel]\\\\  or \\\\\n",
    "b = (A^TA + \\alpha^2 \\cdot I)^{-1}A^Tx$$\n",
    "\n",
    "In this way, the inverse values of small eigenvalues do not blow up when we compute the inverse. You can see this by writing out the $\\Lambda^+$ matrix with the bias term.\n",
    "\n",
    "$$\\Lambda_{Tikhonov}^+  = \\begin{bmatrix}\n",
    "    \\frac{1}{\\lambda_1 + \\alpha^2}  & 0 & 0 & \\dots & 0 \\\\\n",
    "    0  & \\frac{1}{\\lambda_2 + \\alpha^2} & 0 & \\dots & 0 \\\\\n",
    "    \\vdots &\\vdots &\\vdots & & \\vdots \\\\\n",
    "    0 & 0 & 0 & \\dots & \\frac{1}{\\lambda_m + \\alpha^2}\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "Adding this bias term ensures there are no non-zero eigenvalues, and that the inverse of $A^TA$ exists. You can also see that added the bias term adds a 'ridge' along the diagonal of the eigenvalue matrix, giving this method one of its names. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Eigenvalues of the model matrix\n",
    "\n",
    "The foregoing is a bit abstract. To build some intuition you will now investigate the behavior of the eigenvalues for an unregularized and regularized model matrix.   \n",
    "\n",
    "The convergence properties of a linear model can be summarized by the **condition number**, $C = \\frac{abs(largest\\ eigenvalue)}{abs(smallest\\ eigenvalue)}$. Generally, a model matrix with a condition number of less than a few hundred is considered to have good convergence properties. Models with poor convergence properties are unlikely to generalize well. \n",
    "\n",
    "> **Exercise 7-3:** For this exercise you will continue working with the model matrix of the running examples, `x_scale`. You will first investigate the eigenvalues of the unregularized model matrix. Then you will apply regularization to the model matrix and investigate the changes in the eigenvalue. Now do the following:   \n",
    "> 1. Compute the (**real part** of the **unnormaized**) covariance matrix using [numpy.transpose](https://numpy.org/doc/stable/reference/generated/numpy.transpose.html) and [numpy.matmul](https://numpy.org/doc/stable/reference/generated/numpy.matmul.html).\n",
    "> 2. Compute the eigenvalues of the covariance matrix using (numpy.linalg.eigvals)[https://numpy.org/doc/stable/reference/generated/numpy.linalg.eigvals.html]. \n",
    "> 3. Print the eigenvalue vector.  \n",
    "> 4. Log base 10 transform the eigenvalues using (numpy.log10)[https://numpy.org/doc/stable/reference/generated/numpy.log10.html] and plot them vs the number. The eigenvalues are in numbered order. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_eigs(eig_vals):\n",
    "    fig, ax = plt.subplots(figsize=(8,8))\n",
    "    x_vals = [x + 1 for x in range(len(eig_vals))]\n",
    "    ax.plot(x_vals, np.log10(eig_vals));\n",
    "    ax.set_xlabel('eigenvalue number');\n",
    "    ax.set_ylabel('eigenvalue');\n",
    "    ax.set_title('eigenvalue vs. number');\n",
    "\n",
    "## Your code below\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 5. Execute the code in the cell below to create a matrix with the regularization or bias term on the diagonal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diag_reg = np.zeros((covariance.shape), float)\n",
    "np.fill_diagonal(diag_reg, 1.0)\n",
    "diag_reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 6. To create a regularized covariance matrix, add the covariance matrix to the `diag_reg` matrix.  \n",
    "> 7. Compute and print the eigenvalues of the regularized covariance matrix.  \n",
    "> 8. Plot the eigenvalues of the regularized covariance matrix.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Put your code below\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Answer the following questions:   \n",
    "> 1. Examine the eigenvalues of the unregularized model matrix. Taking advantage of the fact that eigenvalues are presented descending order, what is the approximate condition number of the scaled model matrix, what does this tell you about the convergence properties of the model and its generalization properties?   \n",
    "> 2. Examine the eigenvalues of the regularized model matrix. What is the approximate condition number of the regularized model matrix, what does this tell you about the convergence properties of the model and its generalization properties? \n",
    "> 3. Notice that the eigenvalues of the regularized model converge to the value of the regularization coefficient as the eigenvalue number increases. Eigenvalues of about the value of the regularization coefficient may add small amounts of bias to the model predictions but cannot provide any predictive information. Given this consideration, what is the equivalent number of informative features? Note that these 'features' are projections into the eigenspace, not actual columns of the model matrix.  \n",
    "> **End of exercise**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answers:**\n",
    "> 1.  \n",
    "> 2.   \n",
    "> 3.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Regularization for regression  \n",
    "\n",
    "Let's go back to our regression example. Recall that the 9th order polynomial regression model was massively over-fit. Can l2 regularization help this situation? We can create a model applying regularization and find out. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 7-4:** You will now create and test code for an l2 regularized, or ridge, regression model in the cell below using the [Ridge](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html) model from sklearn.linear_model. The Ridge model has an argument `alpha` which corresponds to the regularization parameter, in the notation we have been using. Now, do the following:    \n",
    "> 1. Define a ridge regression model object named, `mod_L2`, with `alpha=1.0` as the L2 regularization parameter.  \n",
    "> 2. Fit the ridge regression model with the same features and labels used previously.  \n",
    "> 3. Apply the predict method using the feature data as the argument. \n",
    "> 5. Use the `plot_reg` to display the fit of the model to the training data.  \n",
    "> 6. Print the model coefficients and RMSE of the model fit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code below\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Compare the results for this model to the un-regularized one trained previously and answer the following questions:   \n",
    "> 1. Compare the graphs of the training data fit and RMSE for the regularized and un-regularized models. How can you explain these differences and what do these differences imply for training bias and generalization of the model.     \n",
    "> 2. Compare the magnitudes of the model coefficients between the regularized and un-regularized models. Do these differences account for the different behavior of the regularized and un-regularized models and why?   \n",
    "> **End of exercise**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answers:**  \n",
    "> 1.    \n",
    "> 2.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let's test the model on the test data. Execute the code in the cell below and examine the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_mod_multi(x_test, x_scale_test, y_test, mod_L2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 7-5:** This result looks a lot more reasonable than for the un-regularized model. How can you best describe the effect of the regularization on the variance and bias and generalization of the model?."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answer:** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 l2 regularization for deep learning models \n",
    "\n",
    "So, you may well wonder, how well l2 regularization applies to deep neural networks? Let's give it a try using the 9th order polynomial data.  \n",
    "\n",
    "The code in the cell below defines and fits regression model with a single hidden layer with 128 units. No regularization is applied in this first model. Execute the code and view the model summary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = models.Sequential()\n",
    "nn.add(layers.Dense(128, activation = 'relu', input_shape = (9, )))\n",
    "nn.add(layers.Dense(1))\n",
    "nn.compile(optimizer = 'rmsprop', loss = 'mse', metrics = ['mae'])\n",
    "history = nn.fit(x_scale, y_train, \n",
    "                  epochs = 100, batch_size = 1,\n",
    "                  validation_data = (x_scale_test, y_test),\n",
    "                   verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the model fit, let's have a look at the loss function vs. training epoch. Execute the code in the cell below and examine the result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_nn_fit(x, y, mod):\n",
    "    predicted = mod.predict(x)\n",
    "    plot_reg(x[:,0], predicted, y)\n",
    "    print('RMSE = ' + str(np.std(predicted - y)))\n",
    "    \n",
    "def plot_loss(history):\n",
    "    train_loss = history.history['loss']\n",
    "    test_loss = history.history['val_loss']\n",
    "    x = list(range(1, len(test_loss) + 1))\n",
    "    plt.plot(x, test_loss, color='red', label='Test loss')\n",
    "    plt.plot(x, train_loss, color='blue', label='Training loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Training Loss')\n",
    "    plt.title('Loss vs. Epoch')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    " \n",
    "plot_loss(history) \n",
    "plot_nn_fit(x_scale, y_train, nn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training and test loss improves for quite a few epochs. \n",
    "\n",
    "Execute the code in the cell below to compute and plot predictions for the unconstrained model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_mod_multi(x_test, x_scale_test, y_test, nn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will try to improve this result by applying l2 norm regularization to the neural network. The code in cell below adds l2 regularization to the model. Execute the code and examine the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_L2 = models.Sequential()\n",
    "nn_L2.add(layers.Dense(128, activation = 'relu', input_shape = (9, ),\n",
    "                        kernel_regularizer=regularizers.l2(10.0)))\n",
    "nn_L2.add(layers.Dense(1))\n",
    "nn_L2.compile(optimizer = 'rmsprop', loss = 'mse', metrics = ['mae'])\n",
    "history = nn_L2.fit(x_scale, y_train, \n",
    "                  epochs = 50, batch_size = 1,\n",
    "                  validation_data = (x_scale_test, y_test),\n",
    "                  verbose = 0)\n",
    "\n",
    "plot_loss(history)\n",
    "plot_nn_fit(x_scale, y_train, nn_L2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But are the predictions any better? Execute the code in the cell below and find out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mod_multi(x_test, x_scale_test, y_test, nn_L2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 7-6:** Examine and compare the results for the regularized and un-regularizzed neural network models and the regularized linear model. Given the relative capacities of these models how can you explain th differences in test fit, variance and bias, between the three models? \n",
    "> **End of exercise**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answer:**      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0 l1 regularization\n",
    "\n",
    "We can also do regularization using other norms. The **l1 regularization** or **Lasso**  method limits the sum of the absolute values of the model coefficients. The l1 norm is sometime know as the **Manhattan norm**, since distance are measured as if you were traveling on a rectangular grid of streets. This is in contrast to the l2 norm that measures distance 'as the crow flies'. \n",
    "\n",
    "We can compute the l1 norm of the weights as follows:\n",
    "\n",
    "$$||W||^1 = \\big( |w_1| + |w_2| + \\ldots + |w_n| \\big) = \\Big( \\sum_{i=1}^n |w_i| \\Big)^1$$\n",
    "\n",
    "where $|x|$ is the absolute value of $x$. \n",
    "\n",
    "Notice that to compute the l1 norm, we raise the sum of the absolute values to the first power.\n",
    "\n",
    "As with l2 regularization, in l1 regularization  we use a penalty term of the l1 norm of the weights. A penalty multiplier, $\\alpha$, determines how much the norm of the coefficient vector constrains values of the weights. The complete loss function then becomes: \n",
    "\n",
    "$$J(W) = J_{MLE}(W) + \\alpha ||W||^1$$\n",
    "\n",
    "You can see a view of this geometric interpretation in Figure 3.1 below.  \n",
    "\n",
    "![](img/L1.jpg)\n",
    "<center> Geometric view of L1 regularization </center>\n",
    "\n",
    "Notice that in Figure 3.1 if $B1 = 0$ then $B2$ has a value at the limit, or vice versa. In other words, using a l1 norm constraint forces some weight values to zero to allow other coefficients to take correct values. In this way, the l1 norm constraint **knocks out** some weights from the model altogether. In contrast to l2 regularization, l1 regularization will drive some coefficients to exactly zero. As a result, L1 optimization typically leads to **sparse models** which generalize well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 l1 regularization example\n",
    "\n",
    "With these ideas in mind, let's apply l1 norm regularization to the 9th order polynomial regression problem. The code in cell below applies l1 regularized or [Lasso](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html) regularization to the linear regression problem. Execute this code and examine the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_L1 = slm.Lasso(alpha = 0.2, max_iter=100000)\n",
    "mod_L1.fit(x_scale, y_train)\n",
    "y_hat_L1 = mod_L1.predict(x_scale)\n",
    "\n",
    "print('RMSE = ' + str(np.std(y_hat_L1 -  y_train)))\n",
    "print(mod_L1.coef_)\n",
    "\n",
    "plot_reg(x_train, y_hat_L1, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, execute the code in the cell below and examine the prediction results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mod_multi(x_test, x_scale_test, y_test, mod_L1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 7-7:** Compare these results to those of the L2 regularized model and answer these questions:  \n",
    "> 1. In terms of the bias and variance, do the L1 and L2 regularized models differ significantly, and why?    \n",
    "> 2. Compare the magnitudes of the coefficients of the L1 an L2 models. WHat do the differences of these values tell you about the different behavior of these models, and why?  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answers:** \n",
    "> 1.    \n",
    "> 2.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Neural network with l1 regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will try l1 regularization with a neural network. The code in the cell below defines, fits and plots a single layer neural network using l1 regularization. Execute this code and examine the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_L1 = models.Sequential()\n",
    "nn_L1.add(layers.Dense(128, activation = 'relu', input_shape = (9, ),\n",
    "                        kernel_regularizer=regularizers.l1(0.1)))\n",
    "nn_L1.add(layers.Dense(1))\n",
    "nn_L1.compile(optimizer = 'rmsprop', loss = 'mse', metrics = ['mae'])\n",
    "history = nn_L1.fit(x_scale, y_train, \n",
    "                  epochs = 50, batch_size = 1,\n",
    "                  validation_data = (x_scale_test, y_test),\n",
    "                  verbose = 0)\n",
    "plot_loss(history)\n",
    "plot_nn_fit(x_scale, y_train, nn_L1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, Execute the code in the cell below to compute and display predicted values from the trained network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mod_multi(x_test, x_scale_test, y_test, nn_L1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 7-8:** Compare these results to the L2 regularized model. Are there any significant differences in terms of RMSE and fit curve? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answer:**     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.0 Early stopping\n",
    "\n",
    "Early stopping is conceptually simple. Early stopping terminates the training of the neural network model at an epoch before it becomes terribly over-fit. That's it! That is the idea of early stopping.\n",
    "\n",
    "In fact, we have already been using early stopping as we create and test the foregoing regularized models. The question here is, how do we automate this process? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Early stopping algorithm\n",
    "\n",
    "The early stopping algorithm simple. This pseudo code shows the basic loop for early stopping on first epoch with a lower performance metric, which is executed after the first training epoch of the model. \n",
    "\n",
    "`Do while TRUE:  \n",
    "    store current model parameters   \n",
    "    update model for epoch  \n",
    "    if(performance_for_epoch < stored_performance_metric)  \n",
    "        return stored_model  \n",
    "    else  \n",
    "        stored_performance_metric = performance_for_epoch   \n",
    "        store_model = model  \n",
    "`  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 How does early stopping work?\n",
    "\n",
    "Early stopping terminates model learning before over-fitting occurs. But how can we interpret this action in terms of the loss function $J(W)_{MLE}$? Figure 4.1 below provides some insight.   \n",
    "\n",
    "![](Figures/EarlyStopping.JPG)\n",
    "<center>**Figure 4.1 Effect of early stopping on $$J(W)_{MLE}$$**</center>\n",
    "\n",
    "On the left side of the diagram you can see contours of the weight norm. On the right are contours  Early stopping terminates training at some model weight norm $||W||^2$. Ideally this is at the point where the training of $J(W)_{MLE}$ starts to over-fit. Thus, we can think of early stopping as analogous to l2 norm regularization where we write the loss function as:\n",
    "\n",
    "$$argmin_W J(W) = J(W)_{MLE} + \\alpha ||W||^2$$\n",
    "\n",
    "where,\n",
    "\n",
    "$\\alpha = $ a regularization parameter controlling the stopping point. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Early stopping example\n",
    "\n",
    "Manually applying early stopping is both computationally inefficient and rather tedious. Fortunately, Keras has a build in capability that allows automation. \n",
    "\n",
    "To implement this early stopping we need to define 2 Keras **callbacks**. Two such callbacks are required:\n",
    "1. The first callback, **EarlyStopping**, is for the early stopping method.\n",
    "2. The second call back **checkpoints** or saves the current model. \n",
    "\n",
    "These callbacks are defined in the form of a **callbacks list**. \n",
    "\n",
    "Notice that the model defined includes l2 regularization. Thus, this model should replicate the performance observed with manual early stopping. To see how this works, examine and then execute the code in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## First define and compile a model. \n",
    "nn_ES = models.Sequential()\n",
    "nn_ES.add(layers.Dense(128, activation = 'relu', input_shape = (9, ),\n",
    "                        kernel_regularizer=regularizers.l2(1.0)))\n",
    "nn_ES.add(layers.Dense(1))\n",
    "\n",
    "nn_ES.compile(optimizer = 'RMSprop', loss = 'mse', metrics = ['mse'])\n",
    "\n",
    "## Define the callback list\n",
    "filepath = 'my_model_file.hdf5' # define where the model is saved\n",
    "callbacks_list = [\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor = 'val_loss', # Use loss to monitor the model\n",
    "        patience = 1 # Stop after one step with lower accuracy\n",
    "    ),\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath = filepath, # file where the checkpoint is saved\n",
    "        monitor = 'val_loss', # Don't overwrite the saved model unless val_loss is worse\n",
    "        save_best_only = True # Only save model if it is the best\n",
    "    )\n",
    "]\n",
    "\n",
    "## Now fit the model\n",
    "history = nn_ES.fit(x_scale, y_train, \n",
    "                  epochs = 100, batch_size = 1,\n",
    "                  validation_data = (x_scale_test, y_test),\n",
    "                  callbacks = callbacks_list,  # Call backs argument here\n",
    "                  verbose = 0)\n",
    "\n",
    "## Visualize the outcome\n",
    "plot_loss(history)\n",
    "plot_nn_fit(x_scale, y_train, nn_ES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the behavior of the loss with training epoch is behaving as with l2 regularization alone. Notice that the training has been automatically terminated at the point the loss function is at its optimum. \n",
    "\n",
    "Let's also have a look that the accuracy vs. epoch. Execute the code in the cell below and examine the result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_accuracy(history):\n",
    "    train_acc = history.history['mse']\n",
    "    test_acc = history.history['val_mse']\n",
    "    x = list(range(1, len(test_acc) + 1))\n",
    "    plt.plot(x, test_acc, color='red', label='Test error')\n",
    "    plt.plot(x, train_acc, color='blue', label='Training error')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Error Rate')\n",
    "    plt.title('Error Rate vs. Epoch')  \n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "plot_accuracy(history)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The curve of test accuracy is consistent with the test loss.\n",
    "\n",
    "The code in the cell below retrieves the best model (by our stopping criteria) from storage, computes predictions and displays the result. Execute this code and examine the results. \n",
    "\n",
    "> **Note:** You may see a warning about a retracing (on the TensorFlow computation graph). This warning may not actually be correct. These is no known resolution and you can see from the [tread in GitHub](https://github.com/tensorflow/tensorflow/issues/34025). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = keras.models.load_model(filepath)\n",
    "test_mod_multi(x_test, x_scale_test, y_test, best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, these results are similar, but a bit worse, than those obtained while manually stopping the training of the l2 regularized neural network.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.0 Dropout regularization\n",
    "\n",
    "All of the regularization methods we have discussed so far, originated long before the current deep neural network era. We will now look at the **dropout regularization** method. Of all widely used regularization methods, dropout is the only one specifically developed for neural networks. The seminal paper, [Dropout: A Simple Way to Prevent Neural Networks from\n",
    "Overfitting by Srivastava et. al](http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf), 2014, is quite readable and provides a lot more detail than is presented here.\n",
    "\n",
    "We have already seen how l1 norm regularization knocks out some model weights. The **dropout method** method regularizes neural networks by creating an **ensemble** of networks with some fraction $p \\lt 1.0$ of the hidden units removed. Ensemble methods are know to be strong regularizers and produce superior results by combining the learning of multiple **weak learners**. \n",
    "\n",
    "The dropout method is somewhat different from other ensemble methods, such as bagging. This reweighting scheme has several advantages:\n",
    "- The model weights for the resulting network are reweighted by the probabilities they are sampled in the various networks of the ensemble. \n",
    "- The memory required to train the model is simply $O(n)$, where n is the number of weights. A bagged model requires  $O(M*n)$, where $M$ is the number of models in the ensemble.\n",
    "- When making predictions in production only one model is used. Whereas, the predictions for each model in the bag must be computed for bagging. \n",
    "\n",
    "To understand this method, let's recall the basic model for a the output of a lth layer in a fully connected network:\n",
    "\n",
    "$$z^{(l+1)}_i = w^{(l+1)}_i \\cdot h^{(l)} + b^{(l+1)}_i\\\\\n",
    "h^{(l+1)}_i = \\sigma(z^{(l+1)}_i)$$\n",
    "\n",
    "where:\n",
    "\n",
    "$\\sigma = $ the activation function. \n",
    "\n",
    "Now, we need to sample the hidden units with probability $p$, in which case we can write:\n",
    "\n",
    "$$r^{(l)}_i \\sim Bernoulli(p)\\\\\n",
    "\\tilde{h}^{(l)}_i = r^{(l)}_i * y^{(l)}\\\\\n",
    "z^{(l+1)}_i = w^{(l+1)}_i \\cdot \\tilde{h}^{(l)}_i + b^{(l+1)}_i\\\\\n",
    "h^{(l+1)}_i = \\sigma(z^{(l+1)}_i)$$\n",
    "\n",
    "where:\n",
    "\n",
    "$r^{(l)}_i =$dropout vector with values $\\{0,1\\}$.\n",
    "\n",
    "To get a feel for what this means in practice examine Figure 5.1. This figure shows a fully connected network with 4 hidden units and a dropout probability $p = 0.5$. \n",
    "\n",
    "![](img/DropoutExample.JPG)\n",
    "![](img/DropoutExample2.JPG)\n",
    "\n",
    "<center>Figure 5.1   \n",
    "Some possible dropouts for a simple fully connected network with p = 0.5</center>\n",
    "\n",
    "Examine Figure 5.1 and notice the following:\n",
    "\n",
    "- There are 6 ways to achieve dropout with exactly 1/2 the units as shown. \n",
    "- No units might dropout with probability $p^4$. \n",
    "- A single unit might drop out with probability $p^3 (1-p)$. \n",
    "- All units might drop out with probability $(1-p)^4$. This case is not admissible so should not be sampled. \n",
    "\n",
    "In fact there are $n^2$ possible dropout patterns for a hidden layer with n units. This scaling quickly leads to a problem. For any realistic size network, it is not possible fully sample all of the possibilities. Instead, we need to use some kind of approximation with a reasonable number of samples. \n",
    "\n",
    "Ideally, we want a model that gives us the posterior probability of $y$, the output, given $x$ the input which we can write $p(y\\ |\\ x)$. If we had infinite computing resources we could Monte Carlo sample this distribution for our neural network. This ideal reference neural network is known as **Bayesian network**. Clearly, for large scale networks it is not possible to compute this result.   \n",
    "\n",
    "We have to settle for a sampled result. We reweight by the probability that a sample is created. Continuing with the notation we used before we can write:\n",
    "\n",
    "$$p(y\\ |\\ x) \\sim \\sum_r p(r) p(y\\ |\\ x, r)$$\n",
    "\n",
    "where,\n",
    "\n",
    "$r = $ the Bernoulli sampled mask vector. \n",
    "\n",
    "Given enough samples the approximation above will converge to the desired probability distribution. However, in practice it has been found that the **geometric mean** of the ensemble converges faster. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation with dropout regularization\n",
    "\n",
    "Some elements of the weight tensor are dropped out of the model at each step of dropout regularization. Given this, how can the partial derivatives required for backpropagation be computed? \n",
    "\n",
    "Recall the dropout algorithm:  \n",
    "\n",
    "$$r^{(l)}_i \\sim Bernoulli(p)\\\\\n",
    "\\tilde{h}^{(l)}_i = r^{(l)}_i * y^{(l)}\\\\\n",
    "z^{(l+1)}_i = w^{(l+1)}_i \\cdot \\tilde{h}^{(l)}_i + b^{(l+1)}_i\\\\\n",
    "h^{(l+1)}_i = \\sigma(z^{(l+1)}_i)$$\n",
    "\n",
    "where:\n",
    "\n",
    "$r^{(l)}_i =$dropout vector with values $\\{0,1\\}$.\n",
    "\n",
    "There are two cases here:\n",
    "- $r^{(l)}_i = 1$, in which case the partial derivative = 1.\n",
    "- $r^{(l)}_i = 0$, in which case the partial derivatives = 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Computing a neural network with dropout regularization\n",
    "\n",
    "With a bit of theory in mind, we will now apply dropout regularization to training a neural network. The code in the cell below defines a neural network with a dropout layer with $p =0.5$. The rest of this network is identical to other networks we have been working with. Execute this code and examine the result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## First define and compile a model with a dropout layer. \n",
    "nn_DO = models.Sequential()\n",
    "nn_DO.add(layers.Dense(128, activation = 'relu', input_shape = (9, )))\n",
    "nn_DO.add(Dropout(0.5)) # Use 50% dropout on this model\n",
    "nn_DO.add(layers.Dense(1))\n",
    "nn_DO.compile(optimizer = 'rmsprop', loss = 'mse', metrics = ['mse'])\n",
    "\n",
    "## Now fit the model\n",
    "history = nn_DO.fit(x_scale, y_train, \n",
    "                  epochs = 40, batch_size = 1,\n",
    "                  validation_data = (x_scale_test, y_test),\n",
    "                  callbacks = callbacks_list,  # Call backs argument here\n",
    "                  verbose = 0)\n",
    "\n",
    "## Visualize the outcome\n",
    "plot_loss(history)\n",
    "plot_nn_fit(x_scale, y_train, nn_DO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The familiar loss plot looks a bit different here. Notice the kinks in the training loss curve. THis is likely a result of the dropout sampling. \n",
    "\n",
    "Execute the code in the cell below and examine the accuracy vs. epoch curves. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_accuracy(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The behavior of the training accuracy curve has a similar appearance to the loss curve in terms of the jagged appearance. \n",
    "\n",
    "Execute the code in the cell below examine the prediction results for this model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = keras.models.load_model(filepath)\n",
    "test_mod_multi(x_test, x_scale_test, y_test, best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results appear similar to those obtained with other regularization methods for neural networks on this problem. While the dropout method is an effective regularizer it is no 'silver bullet'. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.0 Batch Normalization\n",
    "\n",
    "It is often the case that the distribution of output values of some hidden layers changes . The result is that propagated gradients can become near zero, significantly slowing convergence in many cases. We will discuss this **vanishing gradient problem** in another lesson.  \n",
    "\n",
    "In 2015, [Sergey and Szegedy](https://arxiv.org/pdf/1502.03167.pdf) introduced a solution to this problem with their paper **Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift**. The basic idea is simple. A batch normalization layer maintains an exponential moving average estimate of the mean and variance of the outputs of learn. These values are used to normalize the output values of the preceding layer. In other words, the batch normalization layer ensures the distribution of the output values are constant. \n",
    "\n",
    "Let's try an example. The simple neural network model defined in the code cell below includes a batch normalization layer. Also notice that to improve convergence the early stopping has been modified to have a patience of 3. Execute this code.      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use patience of 3\n",
    "callbacks_list = [\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor = 'val_loss', # Use loss to monitor the model\n",
    "        patience = 3 # Stop after one step with lower accuracy\n",
    "    ),\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath = filepath, # file where the checkpoint is saved\n",
    "        monitor = 'val_loss', # Don't overwrite the saved model unless val_loss is worse\n",
    "        save_best_only = True # Only save model if it is the best\n",
    "    )\n",
    "]\n",
    "\n",
    "## Now, define an NN model using batch normalization. \n",
    "## First define and compile a model with a batch normalization layer. \n",
    "nn_BN = models.Sequential()\n",
    "\n",
    "## Fully connected layers with droppout regularization only\n",
    "nn_BN.add(layers.Dense(128, input_shape=(9, )))\n",
    "nn_BN.add(LeakyReLU(alpha=0.2))\n",
    "nn_BN.add(layers.Dense(128)) \n",
    "nn_BN.add(BatchNormalization(momentum = 0.1))\n",
    "nn_BN.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "nn_BN.add(layers.Dense(1))\n",
    "## Define the optimizer and compile\n",
    "optm = tf.keras.optimizers.RMSprop(learning_rate=0.0001)\n",
    "nn_BN.compile(optimizer = optm, loss = 'mse', metrics = ['mse'])\n",
    "\n",
    "## Now fit the model\n",
    "history = nn_BN.fit(x_scale, y_train, \n",
    "                  epochs = 100, batch_size = 1,\n",
    "                  validation_data = (x_scale_test, y_test),\n",
    "                  callbacks = callbacks_list,  # Call backs argument here\n",
    "                  verbose = 0)\n",
    "\n",
    "## Visualize the outcome\n",
    "plot_loss(history)\n",
    "plot_nn_fit(x_scale, y_train, nn_BN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss decreases rapidly and then remains in a narrow range thereafter. It appears that convergence is quite rapid.\n",
    "\n",
    "How does the accuracy evolve with the training episodes? Execute the code in the cell below to display the result.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_accuracy(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This accuracy curve is rather unusual. It seems to reflect the simple regularization being used. \n",
    "\n",
    "Finally, execute the code in the cell below to evaluate the predictions made with this model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = keras.models.load_model(filepath)\n",
    "test_mod_multi(x_test, x_scale_test, y_test, best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite the odd convergence properties of this model the fit to the test data is similar to that achieved with the  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.0 Data augmentation and brittle neural networks\n",
    "\n",
    "From the foregoing examples, you have likely noticed that the simple linear regression models give more satisfactory results than the neural network models. This is the case regardless of regularization method chosen, and is a demonstration of three interrelated and unfortunate properties of neural networks:\n",
    "\n",
    "1. Neural network models have large numbers of parameters (weights). With any finite size data set, there is likely to be a low ratio of cases per parameter. \n",
    "2. As a result of the large numbers of parameters, neural networks are susceptible to noise in the training data.  \n",
    "3. Presumably as a result of the model complexity, neural networks often return unexpected predictions for data cases far from the training data domain. This property has been referred to as **brittleness**. A corollary of this situation is that many deep neural networks are susceptible to **adversarial attacks**. In an adversarial attack, a set of input values outside the range of training cases can lead to unexpected, and sometimes damaging, results. \n",
    "\n",
    "One approach to reducing the brittleness of a neural network is to augment the training data. **Data augmentation** is a process of creating new training cases by adding small perturbations or noise to the original training cases. The desired result is a larger training data set which covers a larger domain of the possible space of features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Prewhitening as data augmentation\n",
    "\n",
    "There are a number of ways to create augmented training data. We will explore other approaches elsewhere. Here, we will use a simple approach of adding **white noise** to existing training cases to create new cases. By white noise we mean broad spectrum noise, specifically Normally distributed noise.  \n",
    "\n",
    "The idea of prewhitening has a long history in various disciplines, such as signal processing, image processing and control theory. The idea is simple. Normally distributed noise is added to the training data to create data cases with lower autocorrelation properties. \n",
    "\n",
    "To get a feel for how data augmentation using prewhitening acts as a regularizer, let's look at the simple case of linear regression. Skipping some tedious algebra we can formulate a white noise augmented linear regression problem. \n",
    "\n",
    "We can draw a array of independent identically distributed (iid) Normal random variables as follows:\n",
    "\n",
    "$$G \\sim Norm(\\mu, \\sigma) $$\n",
    "\n",
    "We can compute the covariance of this vector random variable as follows:\n",
    "\n",
    "$$Cov(G) = G^TG$$\n",
    "\n",
    "Since $G$ is comprised of iid random variables, the off diagonal terms of the covariance matrix will be close to zero. Therefore, the covariance matrix is positive definite, and with no zero eigenvalues. This point is critical. If the augmentation method does not produce iid results, the model will have unwanted collinearities.  \n",
    "\n",
    "If we have a matrix of features $X$ and labels $y$, we can create an augmented model matrix by concatenating a matrix $X + G$ onto the original $X$. The resulting model matrix has the same number of columns but with augmented rows. Since we have constructed $G$ to have a favorable covariance function out augmented model matrix will inherit some so this favorable characteristic.  \n",
    "\n",
    "In the same way, we can augment the label vector to create a longer vector. Again, since the noise we are adding is iid, we expect a favorable covariance structure. Notice that we will need to make sure the order of the augmented features matches the order of the augmented labels. \n",
    "\n",
    "In effect, prewhitening stabilizes the eigenvalues of the covariance matrix. This process is similar to the ridge regression or l2 regularization. As with most other regularization methods, this process introduces bias. The magnitude of the white noise added determines the point on the bias-variance trade-off. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Prewhitened neural network\n",
    "\n",
    "Let's try a computational example. We will train a neural network using white noise augmented data. \n",
    "\n",
    "The code in the cell below performs the following steps:\n",
    "\n",
    "1. A random resampling index is computed by Bernoulli sampling with replacement. \n",
    "2. Augment feature and label array are created by adding Normally distributed noise. \n",
    "3. The original data values are concatenated with the augmented values.\n",
    "4. A plot is made to test the data augmentation.\n",
    "5. The polynomial features are computed and scaled.\n",
    "\n",
    "Execute this code and examine the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create an array of indicies to resample the the \n",
    "## original data with prepacement. \n",
    "nr.seed(3344)\n",
    "size_mult = 4\n",
    "noise = 2.0\n",
    "n = x_train.shape[0]\n",
    "indx = np.array(range(n))\n",
    "indx = nr.choice(indx, size = size_mult*n )\n",
    "\n",
    "## Create augmented data with small amount of noise\n",
    "x_augment = np.add(x_train[indx], normal(scale = noise, size = size_mult*n))\n",
    "y_augment = np.add(y_train[indx], normal(scale = noise, size = size_mult*n))\n",
    "\n",
    "## Check the result\n",
    "print(x_train.shape)\n",
    "plt.scatter(x_train, y_train)\n",
    "plt.scatter(x_augment, y_augment, color = 'red')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.title('Orignial data (blue) and augmented data (red)')\n",
    "\n",
    "## Add in the original data cases and scale\n",
    "x_augment = scale(np.concatenate((x_train, x_augment)))\n",
    "y_augment = np.concatenate((y_train, y_augment))\n",
    "\n",
    "## Compute the polynomial features\n",
    "x_augment = np.power(x_augment.reshape(-1, 1), range(1,10))\n",
    "x_augment = scale(x_augment)\n",
    "\n",
    "print('There are now ' + str(x_augment.shape[0]) + ' cases!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visually, these data seem to have similar statistics to the original data. The augmentation has expanded the domain of the training data. Further, several of the data lie in tight clusters. \n",
    "\n",
    "Now, we are ready to compute the neural network model. The model uses both mild l2 and dropout regularization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## First define and compile a model with a dropout layer. \n",
    "nn_PW = models.Sequential()\n",
    "nn_PW.add(layers.Dense(128, activation = 'relu', input_shape = (9, ),\n",
    "                        kernel_regularizer=regularizers.l2(0.5)))\n",
    "nn_PW.add(Dropout(0.5)) # Use 50% dropout on this model\n",
    "nn_PW.add(layers.Dense(1))\n",
    "nn_PW.compile(optimizer = 'rmsprop', loss = 'mse', metrics = ['mse'])\n",
    "\n",
    "## Now fit the model\n",
    "history = nn_PW.fit(x_augment, y_augment, \n",
    "                  epochs = 50, batch_size = 1,\n",
    "                  validation_data = (x_scale_test, y_test),\n",
    "                  verbose = 0)\n",
    "\n",
    "## Visualize the outcome\n",
    "plot_loss(history)\n",
    "plot_nn_fit(x_scale, y_train, nn_PW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine this plot and notice the point at which over-training appears to start. \n",
    "\n",
    "Let's plot the accuracy just to be sure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_accuracy(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test accuracy curve has similar characteristics to the loss curve. \n",
    "\n",
    "Now, we will compute the predicted values and plot them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_mod_multi(x_test, x_scale_test, y_test, nn_PW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results do not show the odd behavior of the other neural network models. There is still significant bias in the result. Apparently, the data augmentation is a strong regularizer. Still there is significant bias, so this is not the 'silver bullet' either. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.0 Using multiple regularization methods\n",
    "\n",
    "In many cases more than one regularization method is applied. We have already applied early stopping with other regularization methods. In this demonstration we will use four regularization methods at once:\n",
    "- l2 regularization.\n",
    "- l1 regularization.\n",
    "- Early stopping. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the code in the cell below. The four regularization methods are combined. Notice that the combined L1 and L2 regularization are performed with the Keras [l1_l2](https://keras.io/api/layers/regularizers/) function, with two arguments for the L1 and L2 regularization factors.     \n",
    "\n",
    "Execute this code and examine the results.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "filepath = 'my_model_file.hdf5' # define where the model is saved\n",
    "callbacks_list = [\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor = 'val_loss', # Use loss to monitor the model\n",
    "        patience = 4 # Stop after one step with lower accuracy\n",
    "    ),\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath = filepath, # file where the checkpoint is saved\n",
    "        monitor = 'val_loss', # Don't overwrite the saved model unless val_loss is worse\n",
    "        save_best_only = True # Only save model if it is the best\n",
    "    )\n",
    "]\n",
    "\n",
    "\n",
    "## Define a model with regularization  \n",
    "nn_MU = models.Sequential()\n",
    "## Dense layer with both l1 and l2 regularization\n",
    "nn_MU.add(layers.Dense(128, activation = 'relu', input_shape = (9, ),\n",
    "                        kernel_regularizer=regularizers.l1_l2(0.1, 1.0)))\n",
    "nn_MU.add(layers.Dense(1))\n",
    "optm = tf.keras.optimizers.RMSprop(learning_rate=0.0001)\n",
    "nn_MU.compile(optimizer=optm, loss = 'mse', metrics = ['mse'])\n",
    "\n",
    "## Now fit the model\n",
    "history = nn_MU.fit(x_scale, y_train, \n",
    "                  epochs = 510, batch_size = 1,\n",
    "                  validation_data = (x_scale_test, y_test),\n",
    "                  callbacks = callbacks_list,  # Call backs argument here\n",
    "                  verbose = 0)\n",
    "\n",
    "## Visualize the outcome\n",
    "plot_loss(history)\n",
    "plot_nn_fit(x_scale, y_train, nn_MU)\n",
    "plot_accuracy(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can evaluate the predictions the model produces. Execute the code in the cell below and examine the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "best_model = keras.models.load_model(filepath)\n",
    "test_mod_multi(x_test, x_scale_test, y_test, best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 7-9:** Compare the results from this model with the L2 and L1 regularized models. Is there are substantial difference in the RMSE and the fit plots between these models, given the simple model? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answer:**   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Copyright 2018, 2019, 2020, 2021 Stephen F Elston. All rights reserved."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
